# Stroke Prediction
# Πρόβλεψη Εγκεφαλικού (Stroke Prediction) με Machine Learning

## Εισαγωγή

Η παρούσα εργασία έχει ως στόχο την ανάπτυξη ενός μοντέλου τεχνητής νοημοσύνης για την πρόβλεψη του κινδύνου εγκεφαλικού επεισοδίου (stroke) βάσει δημογραφικών, κλινικών και κοινωνικοδημογραφικών χαρακτηριστικών. Χρησιμοποιήθηκε το dataset [Healthcare Stroke Dataset](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset), το οποίο περιέχει πληροφορίες για ασθενείς όπως ηλικία, φύλο, ιστορικό ασθενειών, και συνήθειες (π.χ. κάπνισμα).  
Ο κύριος στόχος είναι η αξιολόγηση και σύγκριση διαφορετικών αλγορίθμων μηχανικής μάθησης, με σκοπό την επιλογή του βέλτιστου μοντέλου που θα προσφέρει υψηλή ακρίβεια και αξιοπιστία στην πρόβλεψη.

---

## Προσέγγιση

### Προεπεξεργασία Δεδομένων

- **Καθαρισμός Δεδομένων:** Αφαιρέθηκαν εγγραφές με ελλιπή ή αμφιλεγόμενα δεδομένα (missing values).  
- **Αντιμετώπιση ανισορροπίας κλάσεων:** Το dataset παρουσιάζει σημαντική ανισορροπία μεταξύ θετικών (stroke) και αρνητικών περιπτώσεων. Χρησιμοποιήθηκε η τεχνική SMOTE (Synthetic Minority Over-sampling Technique) για εξισορρόπηση των δεδομένων, αποφεύγοντας bias προς την πλειονότητα.  
- **Κωδικοποίηση Κατηγορικών Μεταβλητών:** Οι κατηγορικές μεταβλητές όπως το φύλο, το κάπνισμα, και η περιοχή κωδικοποιήθηκαν με one-hot encoding ή label encoding όπου απαιτήθηκε.  
- **Κανονικοποίηση/Κλιμάκωση:** Οι αριθμητικές μεταβλητές (π.χ. ηλικία, BMI) κλιμακώθηκαν με StandardScaler ώστε να έχουν μέση τιμή 0 και τυπική απόκλιση 1, προκειμένου να βελτιωθεί η σταθερότητα και η απόδοση των αλγορίθμων.

### Επιλογή Μοντέλων

Επιλέχθηκαν δημοφιλείς αλγόριθμοι για πρόβλεψη ταξινόμησης με ισχυρή αποδοτικότητα και ευκολία ερμηνείας:

- **XGBoost:** Ένα gradient boosting μοντέλο που συνδυάζει υψηλή απόδοση με ανθεκτικότητα σε ανομοιογενή δεδομένα.  
- **Random Forest:** Για σύγκριση, ως ensemble μέθοδος βασισμένη σε δέντρα αποφάσεων με καλό χειρισμό κατηγορικών μεταβλητών.  
- **Νευρωνικό Δίκτυο (Deep Learning):** Μοντέλο Fully Connected (Dense Layers) για να αξιολογηθεί αν μη γραμμικές αλληλεπιδράσεις βελτιώνουν την πρόβλεψη.

### Εκπαίδευση και Βελτιστοποίηση

- Χρησιμοποιήθηκε η βιβλιοθήκη **Optuna** για αυτόματη αναζήτηση των βέλτιστων υπερπαραμέτρων (hyperparameter tuning) για κάθε μοντέλο.  
- Οι υπερπαράμετροι που βελτιστοποιήθηκαν περιλάμβαναν: learning rate, max_depth, αριθμό δέντρων (estimators) για XGBoost, αριθμό νευρώνων και epochs για το Νευρωνικό Δίκτυο κ.ά.  
- Η διαδικασία περιελάμβανε 5-fold cross-validation για αξιόπιστη εκτίμηση της απόδοσης σε ανεξάρτητα σύνολα δεδομένων.

### Σύγκριση Μοντέλων

- Η αξιολόγηση βασίστηκε σε μετρικές όπως **Accuracy, Precision, Recall, F1-score** και **ROC-AUC**.  
- Το μοντέλο XGBoost επέδειξε την καλύτερη ισορροπία μεταξύ ακρίβειας και ευαισθησίας, καθώς και σταθερότητα στα validation folds.  
- Το Νευρωνικό Δίκτυο είχε παρόμοια απόδοση, αλλά με μεγαλύτερο υπολογιστικό κόστος και απαιτούμενο χρόνο εκπαίδευσης.  
- Η Random Forest έδειξε καλές επιδόσεις, αλλά υστέρησε έναντι του XGBoost.

---

## Ερμηνευσιμότητα

- Χρησιμοποιήθηκαν τεχνικές **SHAP (SHapley Additive exPlanations)** για την ερμηνεία των προβλέψεων του μοντέλου XGBoost.  
- Εντοπίστηκαν ως πιο σημαντικοί παράγοντες: ηλικία, ιστορικό υπέρτασης, καρδιακές παθήσεις, κάπνισμα και BMI.  
- Η οπτικοποίηση SHAP values βοήθησε στην καλύτερη κατανόηση του τρόπου που το μοντέλο λαμβάνει αποφάσεις, σημαντικό για κλινική αποδοχή.

---

## Αποτελέσματα

Το βέλτιστο μοντέλο (XGBoost με βελτιστοποιημένες παραμέτρους) πέτυχε τα εξής:

| Μετρική   | Τιμή  |
|-----------|--------|
| Accuracy  | 0.92   |
| Precision | 0.87   |
| Recall    | 0.81   |
| F1-Score  | 0.84   |
| ROC-AUC   | 0.95   |

- **Οπτικοποιήσεις:**  
  - ROC curve  
  - Confusion matrix  
  - SHAP summary plot

---

## Οδηγίες Εγκατάστασης

1. Κλωνοποίηση αποθετηρίου:
   ```bash
   git clone https://github.com/dimimath/stroke-prediction.git
   cd stroke-prediction
