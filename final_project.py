# -*- coding: utf-8 -*-
"""Final Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ye8HuIhwOFj4iwHKDSoGuZChhzzhgiYq
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


file_path = '/content/api/healthcare-dataset-stroke-data.csv'
df = pd.read_csv(file_path)


print("Dataset Shape:", df.shape)
print("\nFirst 5 rows:\n", df.head())


print("\nMissing values per column:")
print(df.isnull().sum())


print("\nDataset Info:")
print(df.info())

print("\nDataset Description:")
print(df.describe(include='all'))


print("\nImputing missing values for 'bmi' column with median:")

df['bmi'] = df['bmi'].fillna(df['bmi'].median())

print("Missing values after imputation:")
print(df.isnull().sum())

from sklearn.preprocessing import LabelEncoder

categorical_cols = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']


le = LabelEncoder()
for col in categorical_cols:
    df[col] = le.fit_transform(df[col])

print("\nEncoded Categorical Columns:")
print(df[categorical_cols].head())

from sklearn.impute import KNNImputer


num_cols = ['age', 'avg_glucose_level', 'bmi']
imputer = KNNImputer(n_neighbors=5)
df[num_cols] = imputer.fit_transform(df[num_cols])

from sklearn.ensemble import IsolationForest

iso = IsolationForest(contamination=0.01, random_state=42)
outliers = iso.fit_predict(df[num_cols])


df = df[outliers == 1].reset_index(drop=True)

from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()
df[num_cols] = scaler.fit_transform(df[num_cols])

!pip install category_encoders

from category_encoders import TargetEncoder

cat_cols = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']
target_enc = TargetEncoder(cols=cat_cols)
df[cat_cols] = target_enc.fit_transform(df[cat_cols], df['stroke'])

# 1. Age group
df['age_group'] = pd.cut(df['age'], bins=[0, 30, 60, 100], labels=[0, 1, 2])
df['age_group'] = df['age_group'].cat.add_categories([3]).fillna(3)  # αν θέλεις 'other' group

df['age_group'] = df['age_group'].astype(int)

# 2. BMI * Glucose (risk factor combo)
df['bmi_glucose'] = df['bmi'] * df['avg_glucose_level']

# 3. Hypertension & Heart Disease combo
df['ht_hd_combo'] = df['hypertension'] * df['heart_disease']

from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer

X = df.drop(['stroke', 'id'], axis=1)
y = df['stroke']


print("\nMissing values in X before feature selection:")
print(X.isnull().sum())

from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.ensemble import RandomForestClassifier

X = df.drop(['stroke', 'id'], axis=1)
y = df['stroke']

# SelectKBest
kbest = SelectKBest(score_func=f_classif, k=8)
X_kbest = kbest.fit_transform(X, y)
print("SelectKBest features:", X.columns[kbest.get_support()])

# Tree-based importance
rf = RandomForestClassifier(random_state=42)
rf.fit(X, y)
importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)
print("Top Tree-based features:\n", importances.head(8))

from sklearn.decomposition import PCA

pca = PCA(n_components=0.95)
X_pca = pca.fit_transform(X)
print(f"PCA reduced features: {X_pca.shape[1]}")

from sklearn.model_selection import train_test_split, StratifiedKFold

X_train_full, X_test, y_train_full, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y)

X_train, X_valid, y_train, y_valid = train_test_split(
    X_train_full, y_train_full, test_size=0.25, random_state=42, stratify=y_train_full)

from imblearn.over_sampling import SMOTE

sm = SMOTE(random_state=42)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)

print("Before SMOTE:", y_train.value_counts())
print("After SMOTE:", pd.Series(y_train_res).value_counts())

!pip install optuna

from xgboost import XGBClassifier
from sklearn.metrics import roc_auc_score
import optuna
import xgboost as xgb

def objective(trial):
    params = {
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'n_estimators': trial.suggest_int('n_estimators', 50, 300),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),
        'subsample': trial.suggest_float('subsample', 0.5, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),
        'scale_pos_weight': trial.suggest_float('scale_pos_weight', 1, 10),
        'use_label_encoder': False,
        'eval_metric': 'auc'
    }

    model = XGBClassifier(**params)

    model.fit(
        X_train, y_train,
        eval_set=[(X_valid, y_valid)],
        verbose=False
    )

    preds = model.predict_proba(X_valid)[:, 1]
    auc = roc_auc_score(y_valid, preds)
    return auc

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=30)

print("Best parameters:", study.best_params)
print("Best AUC:", study.best_value)

smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

print("After SMOTE:", y_train_res.value_counts())



best_params = study.best_params
best_model = XGBClassifier(**best_params)
best_model.fit(X_train_res, y_train_res)

# Predict probabilities on the original validation set
y_pred_proba = best_model.predict_proba(X_valid)[:, 1]

# ROC Curve & Optimal Threshold
fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba)
optimal_idx = np.argmax(tpr - fpr)
optimal_threshold = thresholds[optimal_idx]
print("Optimal Threshold:", optimal_threshold)

# Apply optimal threshold
y_pred = (y_pred_proba > optimal_threshold).astype(int)

# Evaluation
print("\nClassification Report:\n", classification_report(y_valid, y_pred, zero_division=0))
print("ROC AUC Score:", roc_auc_score(y_valid, y_pred_proba))

# Confusion Matrix
cm = confusion_matrix(y_valid, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ROC Curve Plot
plt.figure()
plt.plot(fpr, tpr, label=f"AUC = {roc_auc_score(y_valid, y_pred_proba):.2f}")
plt.plot([0,1], [0,1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend(loc="lower right")
plt.grid()
plt.show()

from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve
import seaborn as sns
import matplotlib.pyplot as plt

# Προβλέψεις
y_pred_proba = best_model.predict_proba(X_valid)[:, 1] # Use predict_proba to get probabilities
y_pred = (y_pred_proba > 0.5).astype(int)

# Μετρικές
print("Classification Report:\n", classification_report(y_valid, y_pred))
print("ROC AUC:", roc_auc_score(y_valid, y_pred_proba))

# Confusion Matrix
cm = confusion_matrix(y_valid, y_pred)
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix")
plt.show()

fpr, tpr, _ = roc_curve(y_valid, y_pred_proba)
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc_score(y_valid, y_pred_proba):.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

xgb.plot_importance(best_model, max_num_features=10, importance_type='gain')
plt.title('Top 10 Feature Importances')
plt.show()

import shap
import pandas as pd  # Ensure pandas is imported if not already

explainer = shap.Explainer(best_model)
# Use the X_train DataFrame directly with the explainer
shap_values = explainer(X_train)

# Summary Plot
shap.summary_plot(shap_values, X_train, plot_type="bar")

# Detailed summary plot
shap.summary_plot(shap_values, X_train)

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, conint, confloat
import pickle
import numpy as np
import pandas as pd

# Load the trained model (with preprocessing pipeline inside)
with open("model.pkl", "rb") as f:
    model = pickle.load(f)

app = FastAPI(title="Stroke Prediction API")

# Input schema
class StrokeInput(BaseModel):
    gender: str
    age: conint(ge=0, le=120)
    hypertension: conint(ge=0, le=1)
    heart_disease: conint(ge=0, le=1)
    ever_married: str
    work_type: str
    Residence_type: str
    avg_glucose_level: confloat(ge=0)
    bmi: confloat(ge=0)
    smoking_status: str

# Output schema
class StrokeOutput(BaseModel):
    stroke_probability: float
    prediction: int

@app.post("/predict", response_model=StrokeOutput)
def predict(input_data: StrokeInput):
    try:
        # Convert to DataFrame (as during training)
        input_df = pd.DataFrame([input_data.dict()])

        # Apply model (assumes full pipeline: preprocessing + model)
        prediction_proba = model.predict_proba(input_df)[0, 1]
        prediction = model.predict(input_df)[0]

        return StrokeOutput(
            stroke_probability=round(float(prediction_proba), 4),
            prediction=int(prediction)
        )
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Prediction error: {str(e)}")

{
  "gender": "Male",
  "age": 67,
  "hypertension": 0,
  "heart_disease": 1,
  "ever_married": "Yes",
  "work_type": "Private",
  "Residence_type": "Urban",
  "avg_glucose_level": 85.5,
  "bmi": 26.3,
  "smoking_status": "never smoked"
}